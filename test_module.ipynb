{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phamp\\miniforge3\\envs\\PY10CU118\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data import build_dataset, build_sampler, build_dataloader\n",
    "import yaml\n",
    "def yaml_to_dict(path: str):\n",
    "    with open(path) as f:\n",
    "        return yaml.load(f.read(), yaml.FullLoader)\n",
    "config = yaml_to_dict(\"D:\\\\Thesis\\\\DamnShit\\\\Hello\\\\MeMOTR_IKUN\\\\configs\\\\train_mot17_coco.yaml\")\n",
    "config[\"DATA_ROOT\"]=\"D:\\\\Thesis\\\\DamnShit\\\\Hello\\\\MeMOTR_IKUN\\\\DATA_DIR\"\n",
    "config[\"TRAIN_COCO\"]=\"D:\\\\Thesis\\\\DamnShit\\\\Hello\\\\MeMOTR_IKUN\\\\outputs\\\\memotr_mot17_coco\\\\train\\\\mot17_train_coco_reforged.json\"\n",
    "config[\"NO_TRANSFORM\"]=True\n",
    "dataset_train = build_dataset(config=config, split=\"train\")\n",
    "sampler_train = build_sampler(dataset=dataset_train, shuffle=True)\n",
    "dataloader_train = build_dataloader(dataset=dataset_train, sampler=sampler_train,\n",
    "                                    batch_size=config[\"BATCH_SIZE\"], num_workers=config[\"NUM_WORKERS\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=None\n",
    "for i, batch in enumerate(dataloader_train):\n",
    "    if i>1: break\n",
    "    temp=batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(temp):\n",
    "    output=[]\n",
    "    for i,img in enumerate(temp[\"imgs\"][0]):\n",
    "        output_dict=temp[\"infos\"][0][i]\n",
    "        bboxes= output_dict[\"boxes\"]\n",
    "        output_dict[\"local_images\"]=[img.crop(box.numpy()) for box in bboxes]\n",
    "        output_dict[\"global_image\"]=img \n",
    "        output.append(output_dict)\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "check=convert_data(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[ 362.,  105.,  452.,  371.],\n",
       "          [ 362.,  105.,  452.,  371.],\n",
       "          [  22.,  175.,   58.,  271.],\n",
       "          [  77.,  160.,  121.,  278.],\n",
       "          [  22.,  175.,   58.,  271.],\n",
       "          [  77.,  160.,  121.,  278.],\n",
       "          [  -5.,  159.,   44.,  301.],\n",
       "          [  -5.,  159.,   44.,  301.],\n",
       "          [  52.,  172.,   94.,  276.],\n",
       "          [  84.,  113.,  176.,  349.],\n",
       "          [  84.,  113.,  176.,  349.],\n",
       "          [ 186.,  114.,  257.,  349.],\n",
       "          [ 327.,  190.,  352.,  252.],\n",
       "          [ 327.,  190.,  352.,  252.],\n",
       "          [ 346.,   16.,  526.,  488.],\n",
       "          [ 468.,   45.,  616.,  457.],\n",
       "          [ 492., -118.,  799.,  601.],\n",
       "          [ 346.,   16.,  526.,  488.],\n",
       "          [ 468.,   45.,  616.,  457.],\n",
       "          [ 186.,  114.,  257.,  349.],\n",
       "          [  52.,  172.,   94.,  276.],\n",
       "          [ 104.,  173.,  148.,  276.],\n",
       "          [ 104.,  173.,  148.,  276.],\n",
       "          [ 492., -118.,  799.,  601.],\n",
       "          [ 237.,  111.,  329.,  372.],\n",
       "          [ 237.,  111.,  329.,  372.]]),\n",
       "  'ids': tensor([ 71,  71,  84,  89,  84,  89,  90,  90,  96,  91,  91,  95,  92,  92,\n",
       "           93,  94, 128,  93,  94,  95,  96,  98,  98, 128, 129, 129]),\n",
       "  'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]),\n",
       "  'areas': tensor([ 23940,  23940,   3456,   5192,   3456,   5192,   6958,   6958,   4368,\n",
       "           21712,  21712,  16685,   1550,   1550,  84960,  60976, 220733,  84960,\n",
       "           60976,  16685,   4368,   4532,   4532, 220733,  24012,  24012]),\n",
       "  'sentences': ['woman wearing patterned shirt and gray pants',\n",
       "   'woman walking down the street holding a coat on the hand',\n",
       "   'null',\n",
       "   'null',\n",
       "   'person walking on the sidewalk',\n",
       "   'a person walking on the sidewalk',\n",
       "   'woman wearing blue jeans',\n",
       "   'woman walking on the sidewalk',\n",
       "   'woman walking on the sidewalk',\n",
       "   'woman in white tshirt and gray cardigan',\n",
       "   'woman walking on sidewalk',\n",
       "   'woman walking on sidewalk',\n",
       "   'a person in a white shirt',\n",
       "   'a person walking on the street',\n",
       "   'a person walking on the street',\n",
       "   'a person walking on the street',\n",
       "   'a person walking on the street',\n",
       "   'woman with a black purse and black tank top',\n",
       "   'man wearing black shirt and blue jeans',\n",
       "   'woman wearing brown shirt with blue jeans and carrying white purse',\n",
       "   'woman carrying a bag',\n",
       "   'woman wearing black shirt',\n",
       "   'a woman walking on the sidewalk',\n",
       "   'a man wearing a red sweater and black pants',\n",
       "   'woman in blue jeans and black jacket',\n",
       "   'woman walking down the street'],\n",
       "  'dataset': 'MOT17',\n",
       "  'frame_path': 'D:\\\\Thesis\\\\DamnShit\\\\Hello\\\\MeMOTR_IKUN\\\\DATA_DIR\\\\MOT17\\\\images\\\\train\\\\MOT17-05-FRCNN\\\\img1\\\\000785.jpg',\n",
       "  'local_images': [<PIL.Image.Image image mode=RGB size=90x266>,\n",
       "   <PIL.Image.Image image mode=RGB size=90x266>,\n",
       "   <PIL.Image.Image image mode=RGB size=36x96>,\n",
       "   <PIL.Image.Image image mode=RGB size=44x118>,\n",
       "   <PIL.Image.Image image mode=RGB size=36x96>,\n",
       "   <PIL.Image.Image image mode=RGB size=44x118>,\n",
       "   <PIL.Image.Image image mode=RGB size=49x142>,\n",
       "   <PIL.Image.Image image mode=RGB size=49x142>,\n",
       "   <PIL.Image.Image image mode=RGB size=42x104>,\n",
       "   <PIL.Image.Image image mode=RGB size=92x236>,\n",
       "   <PIL.Image.Image image mode=RGB size=92x236>,\n",
       "   <PIL.Image.Image image mode=RGB size=71x235>,\n",
       "   <PIL.Image.Image image mode=RGB size=25x62>,\n",
       "   <PIL.Image.Image image mode=RGB size=25x62>,\n",
       "   <PIL.Image.Image image mode=RGB size=180x472>,\n",
       "   <PIL.Image.Image image mode=RGB size=148x412>,\n",
       "   <PIL.Image.Image image mode=RGB size=307x719>,\n",
       "   <PIL.Image.Image image mode=RGB size=180x472>,\n",
       "   <PIL.Image.Image image mode=RGB size=148x412>,\n",
       "   <PIL.Image.Image image mode=RGB size=71x235>,\n",
       "   <PIL.Image.Image image mode=RGB size=42x104>,\n",
       "   <PIL.Image.Image image mode=RGB size=44x103>,\n",
       "   <PIL.Image.Image image mode=RGB size=44x103>,\n",
       "   <PIL.Image.Image image mode=RGB size=307x719>,\n",
       "   <PIL.Image.Image image mode=RGB size=92x261>,\n",
       "   <PIL.Image.Image image mode=RGB size=92x261>],\n",
       "  'global_image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480>},\n",
       " {'boxes': tensor([[ 362.,  105.,  452.,  371.],\n",
       "          [ 362.,  105.,  452.,  371.],\n",
       "          [  15.,  168.,   47.,  275.],\n",
       "          [  15.,  168.,   47.,  275.],\n",
       "          [ -17.,  160.,   33.,  304.],\n",
       "          [ -17.,  160.,   33.,  304.],\n",
       "          [  45.,  173.,   85.,  278.],\n",
       "          [  70.,  109.,  163.,  357.],\n",
       "          [  70.,  109.,  163.,  357.],\n",
       "          [ 172.,  110.,  249.,  358.],\n",
       "          [ 320.,  189.,  346.,  251.],\n",
       "          [ 320.,  189.,  346.,  251.],\n",
       "          [ 339.,   22.,  525.,  462.],\n",
       "          [ 464.,   42.,  603.,  454.],\n",
       "          [ 493., -118.,  799.,  601.],\n",
       "          [ 339.,   22.,  525.,  462.],\n",
       "          [ 464.,   42.,  603.,  454.],\n",
       "          [ 172.,  110.,  249.,  358.],\n",
       "          [  45.,  173.,   85.,  278.],\n",
       "          [ 104.,  174.,  147.,  277.],\n",
       "          [ 104.,  174.,  147.,  277.],\n",
       "          [ 493., -118.,  799.,  601.],\n",
       "          [ 241.,  112.,  328.,  371.],\n",
       "          [ 241.,  112.,  328.,  371.]]),\n",
       "  'ids': tensor([ 71,  71,  84,  84,  90,  90,  96,  91,  91,  95,  92,  92,  93,  94,\n",
       "          128,  93,  94,  95,  96,  98,  98, 128, 129, 129]),\n",
       "  'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'areas': tensor([ 23940,  23940,   3424,   3424,   7200,   7200,   4200,  23064,  23064,\n",
       "           19096,   1612,   1612,  81840,  57268, 220014,  81840,  57268,  19096,\n",
       "            4200,   4429,   4429, 220014,  22533,  22533]),\n",
       "  'sentences': ['woman wearing patterned shirt and gray pants',\n",
       "   'woman walking down the street holding a coat on the hand',\n",
       "   'null',\n",
       "   'person walking on the sidewalk',\n",
       "   'woman wearing blue jeans',\n",
       "   'woman walking on the sidewalk',\n",
       "   'woman walking on the sidewalk',\n",
       "   'woman in white tshirt and gray cardigan',\n",
       "   'woman walking on sidewalk',\n",
       "   'woman walking on sidewalk',\n",
       "   'a person in a white shirt',\n",
       "   'a person walking on the street',\n",
       "   'a person walking on the street',\n",
       "   'a person walking on the street',\n",
       "   'a person walking on the street',\n",
       "   'woman with a black purse and black tank top',\n",
       "   'man wearing black shirt and blue jeans',\n",
       "   'woman wearing brown shirt with blue jeans and carrying white purse',\n",
       "   'woman carrying a bag',\n",
       "   'woman wearing black shirt',\n",
       "   'a woman walking on the sidewalk',\n",
       "   'a man wearing a red sweater and black pants',\n",
       "   'woman in blue jeans and black jacket',\n",
       "   'woman walking down the street'],\n",
       "  'dataset': 'MOT17',\n",
       "  'frame_path': 'D:\\\\Thesis\\\\DamnShit\\\\Hello\\\\MeMOTR_IKUN\\\\DATA_DIR\\\\MOT17\\\\images\\\\train\\\\MOT17-05-FRCNN\\\\img1\\\\000786.jpg',\n",
       "  'local_images': [<PIL.Image.Image image mode=RGB size=90x266>,\n",
       "   <PIL.Image.Image image mode=RGB size=90x266>,\n",
       "   <PIL.Image.Image image mode=RGB size=32x107>,\n",
       "   <PIL.Image.Image image mode=RGB size=32x107>,\n",
       "   <PIL.Image.Image image mode=RGB size=50x144>,\n",
       "   <PIL.Image.Image image mode=RGB size=50x144>,\n",
       "   <PIL.Image.Image image mode=RGB size=40x105>,\n",
       "   <PIL.Image.Image image mode=RGB size=93x248>,\n",
       "   <PIL.Image.Image image mode=RGB size=93x248>,\n",
       "   <PIL.Image.Image image mode=RGB size=77x248>,\n",
       "   <PIL.Image.Image image mode=RGB size=26x62>,\n",
       "   <PIL.Image.Image image mode=RGB size=26x62>,\n",
       "   <PIL.Image.Image image mode=RGB size=186x440>,\n",
       "   <PIL.Image.Image image mode=RGB size=139x412>,\n",
       "   <PIL.Image.Image image mode=RGB size=306x719>,\n",
       "   <PIL.Image.Image image mode=RGB size=186x440>,\n",
       "   <PIL.Image.Image image mode=RGB size=139x412>,\n",
       "   <PIL.Image.Image image mode=RGB size=77x248>,\n",
       "   <PIL.Image.Image image mode=RGB size=40x105>,\n",
       "   <PIL.Image.Image image mode=RGB size=43x103>,\n",
       "   <PIL.Image.Image image mode=RGB size=43x103>,\n",
       "   <PIL.Image.Image image mode=RGB size=306x719>,\n",
       "   <PIL.Image.Image image mode=RGB size=87x259>,\n",
       "   <PIL.Image.Image image mode=RGB size=87x259>],\n",
       "  'global_image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480>}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'unsuqeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilter_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FilterModule\n\u001b[0;32m      3\u001b[0m filter_module \u001b[38;5;241m=\u001b[39m FilterModule()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mfilter_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\phamp\\miniforge3\\envs\\PY10CU118\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\phamp\\miniforge3\\envs\\PY10CU118\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Thesis\\DamnShit\\Hello\\MeMOTR_IKUN\\models\\mines\\filter_module.py:26\u001b[0m, in \u001b[0;36mFilterModule.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 26\u001b[0m     norm_feats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessing_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m#fusion local_global\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     local_feat\u001b[38;5;241m=\u001b[39mnorm_feats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_images\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\Thesis\\DamnShit\\Hello\\MeMOTR_IKUN\\models\\mines\\filter_module.py:51\u001b[0m, in \u001b[0;36mFilterModule.processing_input\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m global_image \u001b[38;5;241m=\u001b[39m global_image\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mlen\u001b[39m(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_images\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     44\u001b[0m processed\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_images\u001b[39m\u001b[38;5;124m\"\u001b[39m:[ \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(i)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_images\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# \"global_images\":torch.vstack([ self.preprocess(x[\"global_image\"]) for _ in x[\"local_images\"]]).view(-1,3,224,224).to(self.device),\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentences\u001b[39m\u001b[38;5;124m\"\u001b[39m:clip\u001b[38;5;241m.\u001b[39mtokenize(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentences\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     48\u001b[0m }\n\u001b[0;32m     50\u001b[0m feats\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_images\u001b[39m\u001b[38;5;124m\"\u001b[39m:torch\u001b[38;5;241m.\u001b[39mstack(\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprocessed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal_images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsuqeeze\u001b[49m(),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# \"global_images\":self.clip.encode_image(processed[\"global_images\"]),\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentences\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip\u001b[38;5;241m.\u001b[39mencode_text(processed[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentences\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     54\u001b[0m }\n\u001b[0;32m     55\u001b[0m norm_feats\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_images\u001b[39m\u001b[38;5;124m\"\u001b[39m:feats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_images\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m/\u001b[39mfeats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_images\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# \"global_images\":feats[\"global_images\"]/feats[\"global_images\"].norm(dim=-1, keepdim=True),\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal_images\u001b[39m\u001b[38;5;124m\"\u001b[39m:global_image,\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentences\u001b[39m\u001b[38;5;124m\"\u001b[39m:feats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentences\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m/\u001b[39mfeats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentences\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     60\u001b[0m }\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m norm_feats\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'unsuqeeze'"
     ]
    }
   ],
   "source": [
    "from models.mines.filter_module import FilterModule\n",
    "\n",
    "filter_module = FilterModule()\n",
    "# filter_module(check[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
